{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c34f3bcb",
   "metadata": {},
   "source": [
    "## Detecting and mitigating Age and Sex bias on credit decisions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185feb29",
   "metadata": {},
   "source": [
    "### Step 1 -  Importing the Required Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "17d1cb69",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aif360.datasets import GermanDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21168a",
   "metadata": {},
   "source": [
    "### Step 2 - Load dataset, specifying protected attribute, and split dataset into train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2d5bef40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig = GermanDataset(\n",
    "    protected_attribute_names=['age'],                           \n",
    "    privileged_classes=[lambda x: x >= 25],     \n",
    "    features_to_drop=['personal_status', 'sex'] \n",
    "   )\n",
    "\n",
    "dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b2671d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'age': 1}]\n",
    "unprivileged_groups = [{'age': 0}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3988379b",
   "metadata": {},
   "source": [
    "### Step 3 - Compute fairness metric on original training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "75a16d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.154574\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d0dff",
   "metadata": {},
   "source": [
    "### Step 4 - Mitigate bias by transforming the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d555bee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eac1df9",
   "metadata": {},
   "source": [
    "### Step 5 - Compute fairness metric on transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "44d0e287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.000000\n"
     ]
    }
   ],
   "source": [
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5074db48",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Taking the reference of the tutorial and teh above code and following the same steps.\n",
    "\n",
    "- I made changes to the dataset and re-evaluated it using the same measurement method as the original training dataset.\n",
    "- I utilized the BinaryLabelDatasetMetric class and the mean difference method to determine if any bias had been reduced. \n",
    "- The results showed that my mitigation measures were effective, as the difference in mean outcomes had decreased to 0.0.\n",
    "- Previously, the affluent group had a 15 percent advantage, but now the results indicate equality between the two groups in terms of mean outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe960f",
   "metadata": {},
   "source": [
    "###  Using sex attribute to detect and mitigate bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2e7ee2",
   "metadata": {},
   "source": [
    "Taking the reference of the tutorial and teh above code and following the same steps.\n",
    "\n",
    "I loaded the dataset and specified the sex property as the protected attribute. Since age was not necessary for our analysis, it was dropped from the dataset. Next, I divided the original dataset into training and testing sets. To minimize bias, I set the privileged variable \n",
    "- (1) as male and the unprivileged variable \n",
    "- (0) as female for the sex property. \n",
    "\n",
    "These two variables were used to identify and minimize any potential biases in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4d8b47dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_s = GermanDataset(\n",
    "    protected_attribute_names=['sex'],\n",
    "     privileged_classes=[lambda x: x == 'male'],     \n",
    "    features_to_drop=['personal_status', 'age'] \n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8668477a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train_s, dataset_orig_test_s = dataset_orig_s.split([0.7], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "85b3e9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'sex': 1}]\n",
    "unprivileged_groups = [{'sex': 0}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "93126116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.058012\n"
     ]
    }
   ],
   "source": [
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train_s, \n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_orig_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54dd4281",
   "metadata": {},
   "source": [
    "Taking the reference of the tutorial and teh above code and following the same steps.\n",
    "- I found that the privileged group (males) had a 6% advantage in positive outcomes in the training dataset. \n",
    "- This bias was undesirable and needed to be addressed. To mitigate this bias in the training dataset, I used a pre-processing mitigation technique. \n",
    "- This involved considering males as the positive class with a 6% bias and taking steps to minimize this bias in subsequent analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39933412",
   "metadata": {},
   "source": [
    "### Step 4i - Mitigate bias by transforming the original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e87ea",
   "metadata": {},
   "source": [
    "- To address the gender bias present in the original dataset, I applied a pre-processing technique known as reweighing. \n",
    "- This involved assigning different weights to entities in the dataset to ensure fairness and minimize the effects of the bias.\n",
    "\n",
    "- By using this technique, I aimed to reduce the impact of gender on the outcomes of the analysis and create a more equitable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "54d9e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf_train = RW.fit_transform(dataset_orig_train_s) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9008224f",
   "metadata": {},
   "source": [
    "- The algorithm I used to mitigate biasness was the Reweighing Algorithm, which I had applied before building the model. \n",
    "- This algorithm transformed the dataset by assigning different weights to entities to ensure fairness and equity in positive outcomes on the protected attribute for both the privileged and unprivileged groups. \n",
    "- By applying this technique, I aimed to create a more balanced and unbiased dataset that would lead to more accurate and reliable analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b515cec",
   "metadata": {},
   "source": [
    "### Step 5i - Compute fairness metric on transformed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9b205aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in mean outcomes between unprivileged and privileged groups = -0.000000\n"
     ]
    }
   ],
   "source": [
    "metric_transf_train = BinaryLabelDatasetMetric(dataset_transf_train, \n",
    "                                               unprivileged_groups=unprivileged_groups,\n",
    "                                               privileged_groups=privileged_groups)\n",
    "\n",
    "print(\"Difference in mean outcomes between unprivileged and privileged groups = %f\" % metric_transf_train.mean_difference())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239cf2db",
   "metadata": {},
   "source": [
    "- After applying the reweighing technique to mitigate bias in the dataset, I evaluated its effectiveness using the same measures as the original training dataset. \n",
    "- The results showed that the mitigation step was successful, as the difference in mean outcomes had decreased to 0.0.\n",
    "- Previously, the privileged group had a 6% advantage, but now the results indicate equality between the two groups in terms of mean outcomes. \n",
    "- This suggests that the pre-processing technique of reweighing was effective in reducing the bias and creating a more fair and equitable dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdb3c55",
   "metadata": {},
   "source": [
    "- In my analysis, I identified that historical bias in datasets could result in unfair and unjust findings when building models based on that data. \n",
    "- Specifically, in my scenario, males were more likely to receive greater resources due to traditional biases in the data. \n",
    "- This is because traditional machine learning approaches prioritize accuracy over fairness, leading to biased results. \n",
    "- Nevertheless, I also demonstrated how basic bias mitigation strategies like reweighing can be implemented to eliminate bias from datasets, leading to models with equal accuracy and significantly higher fairness measures. \n",
    "- These bias mitigation techniques are crucial for any organization that seeks to automate decision-making processes for populations with protected characteristics and ensure that the resulting models are fair and unbiased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e21372",
   "metadata": {},
   "source": [
    "### Understanding of Mitigation and Bias\n",
    "\n",
    "As a human, I strive to make informed decisions by carefully considering the potential benefits and drawbacks of different options. However, sometimes our instincts may guide us towards certain decisions, which may not always be the best ones. Despite our belief that decision-making is a rational process, research has shown that implicit biases can subconsciously influence our conclusions without our awareness. This can have significant implications for learning leaders within an organization, who may be unaware of the biases that are impacting their objectivity and fairness.\n",
    "Similarly, when dealing with imbalanced datasets, we often have a majority class in the target variable. To address this issue, we can use various machine learning techniques to generate a proper training dataset and reduce bias in our models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a3a81",
   "metadata": {},
   "source": [
    "### Challenges Encountered\n",
    "During the course of this analysis, I encountered several challenges. Firstly, the decision to assume that males were more privileged than females based on their sex was a potential issue since it could be viewed as a biased assumption. Choosing the opposite assumption and assuming females to be more positively biased could have been another option.\n",
    "\n",
    "Additionally, I faced technical challenges with the libraries used for implementing AI fairness measures, which required the installation of TensorFlow for proper functioning. Reading the dataset and making certain choices was also difficult, and I referred to external sources like the UCI Machine Learning Repository for help.\n",
    "\n",
    "Lastly, while loading the dataset, I encountered issues with the dataset location as it needed to be in the conda environment dataset folder, which was not initially the case. I had to download the dataset and place it in the specified path to proceed with the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcbe2cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
